# Running ORCA on the Cluster

Once you've prepared an ORCA input file, the next step is to **run the calculation**. There are multiple ways to do this on the MSU HPC cluster, depending on your workflow.

In this guide, weâ€™ll cover:

- Running ORCA directly from the terminal (basic)
- Running ORCA with a Slurm script (recommended generally)
- Running ORCA using the `sorca`->`runorca` wrapper (automated workflow; **RECOMMENDED**)

---

## 1. Running ORCA Directly (Simple Test)

You can run a small ORCA job interactively (not recommended for real jobs):

Go to your water directory and make sure you have `water.inp`

MSUNHPC uses modules for its software. In order to use the proper version and build of `orca` we need to load that module.

To check the available modules:

`module avail`

To load the orca:

`module load orca`

To see which modules are currently loaded on your session:

`module list`

See that you have loaded the proper orca:
`which orca`

should output

`/shared/apps/orca/6.0.1_ompi_4.1.6/orca`

If you had not loaded the module it would have outputed:

`/usr/bin/orca` which is the wrong version.

Running an orca calculation:

```
`which orca` water.inp > water.out
```
You will not be able to interact with the terminal until the process completes (unless you added an ampersand `&` at the end of the command to put the process in the background). Nevertheless, this method is highly discouraged (except for very simple and quick check runs) as this utilizes the resources of the `headnode` which is very minimal compared to the resources available in the compute nodes

Explore the results after the calculation completes.

## 2. Running ORCA with a slurm file

Create another directory in the directory that you are currently in called `slurm` and go in that subdirectory.

Copy the `water.inp` file from the directory above:

`cp ../water.inp .`

Create a `water.slurm` using `vim` and paste the following:

```
#!/bin/bash
#SBATCH --job-name=water
#SBATCH --output=water.out
#SBATCH --error=water.err
#SBATCH --nodes=1
#SBATCH --ntasks=4
#SBATCH --cpus-per-task=1
#SBATCH --mem=2000
#SBATCH --time=01:00:00
#SBATCH --partition=gpu

# Load ORCA and MPI
module purge
module load orca
module load openmpi 

# Run ORCA
`which orca` water.inp > water.log
```

After saving the file, change its type to an executable file:

`chmod +x water.slurm`

Check that you have two file in this current directory and that the permissions reflect what is shown below:

`ls -l`

Expected Output:
```
-rw-rw-r-- 1 benjoerey benjoerey 237 Apr  9 23:36 water.inp
-rwxrwxr-x 1 benjoerey benjoerey 371 Apr 10 14:33 water.slurm
```

Then submit the job:

`sbatch water.slurm`

Now you have submitted your first slurm job (at least in MSUNHPC... ;p)


## 3. Running ORCA with a wrapper script

The issue with running with a slurm file (specifically for Orca), is that scratch files are all deposited in the directory where you submitted the job. This means that the File I/O happens in the directory where you submitted job. While this is technically okay, in the scenarion where several jobs are running, these I/O can cause bottlenecks. This is why running Orca in an HPC generally is recommended to be done with a wrapper script that

- Manages I/O utilizing `/tmp` disks for localizing files in the compute node
- Performs cleanup and archiving

Below are two wrapper script that are currently implemented in MSUNHPC:

1. `sorca` --> reusable script for job submission
```
#!/bin/bash

# Usage: sorca jobname ncores
jobname=$1
ncores=$2
nodes=1
cpn=$(expr $ncores / $nodes)
mem=2000                      #
days=00
hours=08
min=00
partition=cpu
workdir=$PWD

# Path to runorca script
RUNORCA=/shared/scripts/orca/runorca

# Submit job
sbatch -n $ncores -N $nodes \
       -c 1 \
       --ntasks-per-node=$cpn \
       -p $partition \
       --mem-per-cpu=$mem \
       -J $jobname \
       -t $days-$hours:$min \
       -e $workdir/$jobname.err \
       -o $workdir/$jobname.log \
       $RUNORCA $jobname $workdir
```

2. runorca --> the main wrapper script
```
#!/bin/bash

# Load modules
module load openmpi
module load orca

# Capture arguments
jobname="$1"
submitdir="$2"
hostname=$(hostname)
scratch_root="/tmp"
scratch_dir="${scratch_root}/${USER}_${SLURM_JOB_ID}_${jobname}"
scratch_archive="/shared/scratch/orca/${USER}"
total_mem=$((${SLURM_MEM_PER_CPU} * ${SLURM_NTASKS}))

# Set NBO paths
export NBOBIN=/shared/apps/nbo7/bin
export GENEXE=${NBOBIN}/gennbo.i8.exe
export NBOEXE=${NBOBIN}/nbo7.i8.exe

echo -e "[INFO $(date '+%Y-%m-%d %H:%M:%S')] Starting an Orca calculation from ${submitdir}" > "${submitdir}/${jobname}.log"
echo -e "\nThis job by ${USER} run in ${hostname} requested ${SLURM_NTASKS} cores at ${SLURM_MEM_PER_CPU}MB each [TOTAL MEM: ${total_mem} MB]\n" >> "${submitdir}/${jobname}.log"
echo "[INFO $(date '+%Y-%m-%d %H:%M:%S')] Creating scratch directory at ${scratch_dir}" >> "${submitdir}/${jobname}.log"
mkdir -p "${scratch_dir}"
cd "${scratch_dir}" || exit 1

# Copy input and other supporting files
echo "[INFO $(date '+%Y-%m-%d %H:%M:%S')] Copying input and related files from ${submitdir}" >> "${submitdir}/${jobname}.log"
cp "${submitdir}/${jobname}.inp" job.inp
[[ -f "${submitdir}/${jobname}.gbw" ]] && cp "${submitdir}/${jobname}.gbw" .
[[ -f "${submitdir}/${jobname}.xyz" ]] && cp "${submitdir}/${jobname}.xyz" .
[[ -f "${submitdir}/${jobname}.hess" ]] && cp "${submitdir}/${jobname}.hess" .
cp "${submitdir}"/*.loc . 2>/dev/null

# Suppress OpenMPI fork warning
export OMPI_MCA_mpi_warn_on_fork=0

# Run ORCA
echo -e "[INFO $(date '+%Y-%m-%d %H:%M:%S')] Starting ORCA calculation" >> "${submitdir}/${jobname}.log"
$(which orca) job.inp >> "${submitdir}/${jobname}.log" 2>&1

# Save important files to scratch archivie
archive_dir="${scratch_archive}/${jobname}_${SLURM_JOB_ID}"
echo -e "\n\n[INFO $(date '+%Y-%m-%d %H:%M:%S')] Archiving potentially important files to ${archive_dir}" >> "${submitdir}/${jobname}.log"
mkdir -p "${archive_dir}"
mv job.* *.gbw *.xyz *.hess *.property.txt *.trj *.engrad *.loc *.3* *.4* "${archive_dir}" 2>/dev/null

# Cleanup
echo -e "[INFO $(date '+%Y-%m-%d %H:%M:%S')] Cleaning up scratch" >> "${submitdir}/${jobname}.log"
rm -rf "${scratch_dir}"

# Post-run resource info
echo -e "\n--- Slurm Resource Usage Summary for ${jobname} (JobID: ${SLURM_JOB_ID}) ---" >> "${submitdir}/${jobname}.log"
sacct -j "${SLURM_JOB_ID}" --format=JobID,JobName%15,Elapsed,TotalCPU,AveRSS,MaxRSS,AveVMSize,MaxVMSize >> "${submitdir}/${jobname}.log"
```

**Note:** No need to produce these files as there are already set up.

Let's now use these scripts.

Go back one directory level

`cd ..`

You should be back to the `water` directory (Check using `pwd`). Here create another directory called `wrapper` and copy the `water.inp` file into the `wrapper` directory. 

Go inside the `wrapper` directory.

Check that you only have one file.

Now, submit the job using our wrapper script `sorca`.

`sorca water 4`

Check job completion.

Moving forward, we will be using this `sorca` to submit jobs.

## Exercise

Pick a molecule of your choosing and run an optimization calculation. (P.S., let's limit to small molecules)
